{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNz+UoGyk26HaVT7QYGRMlE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunssup/Business_Text_Mining/blob/main/Tokenization_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "> <b>1. spaCy <br>\n",
        "> <b>2. NLTK <br>\n",
        "> <b>3. Regex </b>\n",
        "\n",
        "# <font color=blue>1. spaCy</font>\n",
        "\n"
      ],
      "metadata": {
        "id": "Vf0hGHqes8Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "_p2gF0n7tAnG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string1 = '\"we\\'re moving to L.A.!\"'\n",
        "print(string1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZsgfxlDtcRy",
        "outputId": "93ebe029-afa5-451e-f35a-d88992507a74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"we're moving to L.A.!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(string1)\n",
        "\n",
        "for token in doc1:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj91WmApt2-o",
        "outputId": "3030b2a9-0a97-45de-abdf-7a0fe5bdab5f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\n",
            "we\n",
            "'re\n",
            "moving\n",
            "to\n",
            "L.A.\n",
            "!\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spaCy는 단어의 필수적인 부분을 형성하지 않는 문장을 분리**합니다. 문장 끝의 따옴표, 쉼표, 문장은 각각의 토큰을 할당합니다.\n",
        "\n",
        " 단, 이메일 주소, 웹사이트 또는 수치의 일부로 존재하는 구두점은 토큰의 일부로 유지됩니다."
      ],
      "metadata": {
        "id": "LS_i6A5euRwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string2 = \"AT&T offers a $49.99 montly plan for N.Y. citizens who enjoy rock'n'roll.\"\n",
        "\n",
        "doc2 = nlp(string2)\n",
        "for token in doc2:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWRmB4CluDJT",
        "outputId": "63870381-08ff-4014-f08a-277caaea7333"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AT&T\n",
            "offers\n",
            "a\n",
            "$\n",
            "49.99\n",
            "montly\n",
            "plan\n",
            "for\n",
            "N.Y.\n",
            "citizens\n",
            "who\n",
            "enjoy\n",
            "rock'n'roll\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string3 = \"If you'd like to join Ph.D. program, email us at userid@dongguk.edu or visit at https://www.dongguk.edu\"\n",
        "doc3 = nlp(string3)\n",
        "for token in doc3:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZCX8vquukiI",
        "outputId": "2ad7b1a6-e468-475f-c247-3ed1a097afc7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If\n",
            "you\n",
            "'d\n",
            "like\n",
            "to\n",
            "join\n",
            "Ph.D.\n",
            "program\n",
            ",\n",
            "email\n",
            "us\n",
            "at\n",
            "userid@dongguk.edu\n",
            "or\n",
            "visit\n",
            "at\n",
            "https://www.dongguk.edu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62VopbicuxVf",
        "outputId": "527b0d47-6ac5-4f04-e0cd-630a47e1e73b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=blue> 2. NLTK </font>\n",
        "> - word_tokenize\n",
        "> - WordPunctTokenizer\n",
        "> - TweetTokenizer"
      ],
      "metadata": {
        "id": "AUaFymYnu74e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o73p8MTtu3Zo",
        "outputId": "a87dd349-10a5-4d5e-fd4b-5e338d1006b4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. word_tokenize()\n",
        "<b> Example 1. </b><br>\n",
        "- string = '\"we\\'re moving to L.A.!\"'"
      ],
      "metadata": {
        "id": "xBLAtOFKvfvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "oUqHHQ7avB6C"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(string1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdZFutlDvlpE",
        "outputId": "e5518c6d-8a80-4bc6-88d0-e49698d24398"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['``', 'we', \"'re\", 'moving', 'to', 'L.A.', '!', \"''\"]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc1:\n",
        "    print(token.text, end=' | ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lPbe5nyvp3I",
        "outputId": "28b9c228-893a-4e6b-9109-fe7f77701988"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" | we | 're | moving | to | L.A. | ! | \" | "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. word_tokenize()\n",
        "<b> Example 2. </b><br>\n",
        "- string = \"AT&T offers a $49.99 montly plan for N.Y. citizens who enjoy rock'n'roll.\""
      ],
      "metadata": {
        "id": "mPU4zCUpvvNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(string2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_Yo9DIPvsd9",
        "outputId": "4d95a1f5-9c6a-43d3-8785-2533adaadf19"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AT',\n",
              " '&',\n",
              " 'T',\n",
              " 'offers',\n",
              " 'a',\n",
              " '$',\n",
              " '49.99',\n",
              " 'montly',\n",
              " 'plan',\n",
              " 'for',\n",
              " 'N.Y.',\n",
              " 'citizens',\n",
              " 'who',\n",
              " 'enjoy',\n",
              " \"rock'n'roll\",\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc2:\n",
        "    print(token.text, end=' | ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAcS6aWRvy6O",
        "outputId": "5c71442e-740f-4e0f-cd3c-a514c6383b12"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AT&T | offers | a | $ | 49.99 | montly | plan | for | N.Y. | citizens | who | enjoy | rock'n'roll | . | "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. word_tokenize()\n",
        "<b> Example 3. </b><br>\n",
        "- string = \"If you'd like to join Ph.D. program, email us at userid@dongguk.edu or visit at https://www.dongguk.edu\""
      ],
      "metadata": {
        "id": "GGxyQaW6v9dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(string3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpglJfAYv1c0",
        "outputId": "9c553b0b-ff8e-41ab-a326-091fb758426c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['If',\n",
              " 'you',\n",
              " \"'d\",\n",
              " 'like',\n",
              " 'to',\n",
              " 'join',\n",
              " 'Ph.D.',\n",
              " 'program',\n",
              " ',',\n",
              " 'email',\n",
              " 'us',\n",
              " 'at',\n",
              " 'userid',\n",
              " '@',\n",
              " 'dongguk.edu',\n",
              " 'or',\n",
              " 'visit',\n",
              " 'at',\n",
              " 'https',\n",
              " ':',\n",
              " '//www.dongguk.edu']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc3:\n",
        "  print(token.text, end = ' | ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CAyiIlKwBQA",
        "outputId": "95eec074-9ef8-4e6c-8e82-681eeb4cf29b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If | you | 'd | like | to | join | Ph.D. | program | , | email | us | at | userid@dongguk.edu | or | visit | at | https://www.dongguk.edu | "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. WordPunctTokenizer"
      ],
      "metadata": {
        "id": "uHTPQ-gcwOya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer"
      ],
      "metadata": {
        "id": "ZvmCdb2zwJMB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punct_tokenizer = WordPunctTokenizer()\n",
        "punct_tokenizer.tokenize(string3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdFKpGwdwUud",
        "outputId": "47887dfc-06f3-4ae5-c3c2-4213f17d9b90"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['If',\n",
              " 'you',\n",
              " \"'\",\n",
              " 'd',\n",
              " 'like',\n",
              " 'to',\n",
              " 'join',\n",
              " 'Ph',\n",
              " '.',\n",
              " 'D',\n",
              " '.',\n",
              " 'program',\n",
              " ',',\n",
              " 'email',\n",
              " 'us',\n",
              " 'at',\n",
              " 'userid',\n",
              " '@',\n",
              " 'dongguk',\n",
              " '.',\n",
              " 'edu',\n",
              " 'or',\n",
              " 'visit',\n",
              " 'at',\n",
              " 'https',\n",
              " '://',\n",
              " 'www',\n",
              " '.',\n",
              " 'dongguk',\n",
              " '.',\n",
              " 'edu']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. TweetTokenizer\n",
        "\n",
        "'#'태그와 @ 태그 및 기호가 별도의 토큰으로 분리"
      ],
      "metadata": {
        "id": "nY6Pjfsdwd6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = \"Snow White and the Seven Degrees #MakeAMovieCold@midnight :-)\""
      ],
      "metadata": {
        "id": "JRablSP4wWxI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- spacy"
      ],
      "metadata": {
        "id": "3T7AGFwEw3Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(tweet)\n",
        "for token in doc:\n",
        "  print (token.text, end = ' | ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr0M-nS_witC",
        "outputId": "26fb93ad-003e-4cbb-9328-7a441f048ad9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snow | White | and | the | Seven | Degrees | # | MakeAMovieCold@midnight | :-) | "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TweetTokenizer"
      ],
      "metadata": {
        "id": "Ra0byfnvw43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "print(tokenizer.tokenize(tweet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaTScR-rwrD2",
        "outputId": "bf41748a-5609-481e-ed40-eebf619cd99e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Snow', 'White', 'and', 'the', 'Seven', 'Degrees', '#MakeAMovieCold', '@midnight', ':-)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=blue> 3. Tokenization using Regex </font>"
      ],
      "metadata": {
        "id": "gk5yW_Uow_Ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"The quick brown   fox jumps over the    lazy dog\"\n",
        "re.split(\"\\s+\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__vrUJUcw-7i",
        "outputId": "bc0a27d9-0735-4d91-8277-e3ec765ea155"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "metadata": {
        "id": "MnMag0Uuw9ru"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RegexpTokenizer는 NLTK(Natural Language Toolkit) 라이브러리에서 제공하는 정규 표현식을 기반으로 한 토크나이저"
      ],
      "metadata": {
        "id": "9t_d3cJwxmaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"AT&T offers a $49.99 montly plan for N.Y. citizens\""
      ],
      "metadata": {
        "id": "I6GmJ-pbxGJD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1 = RegexpTokenizer(\"\\w+\")\n",
        "print(tokenizer1.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAoDc19mxIRo",
        "outputId": "7044357d-8119-4d76-ed1a-1f8485e63ccc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AT', 'T', 'offers', 'a', '49', '99', 'montly', 'plan', 'for', 'N', 'Y', 'citizens']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫 번째 토크나이저는 **\\w+라는 정규 표현식**을 사용. 여기서 **\\w는 \"word character\"를 의미**하며, **보통 영문자, 숫자, 밑줄 문자(_)를 포함**. **+는 하나 이상의 문자에 일치한다는 것을 의미**.\n",
        "\n",
        "따라서 이 패턴은 연속된 \"단어 문자들\"을 찾아내어 토큰으로 분리. 그러나 이 방식은 **공백, 특수 문자, 구두점 등을 기준으로는 분리하지 않기** 때문에, 예를 들어 \"N.Y.\" 같은 경우는 \"N\", \"Y\"로 분리."
      ],
      "metadata": {
        "id": "AuGJJAhjxyAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2 = RegexpTokenizer(\"\\s+\", gaps=True)\n",
        "print(tokenizer2.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBY957eHxJ4P",
        "outputId": "b3013df4-d2d7-4093-893c-270b65c5c113"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AT&T', 'offers', 'a', '$49.99', 'montly', 'plan', 'for', 'N.Y.', 'citizens']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 번째 토크나이저는 **\\s+, 즉 하나 이상의 공백 문자를 기준으로 텍스트를 토큰화**하는 정규 표현식을 사용. 여기서 gaps=True 매개변수는 지정된 패턴을 토큰으로 분리하는 대신 토큰을 구분하는 구분자로 사용하라는 의미. 즉, 이 경우에는 **공백이 토큰들을 구분하는 구분자로 작용**하여, **공백을 기준으로 텍스트를 분리**합니다. 이 방식은 단어뿐만 아니라 **특수 문자와 구두점도 포함된 토큰을 생성 가능**."
      ],
      "metadata": {
        "id": "TkHLvlQkyDJ2"
      }
    }
  ]
}